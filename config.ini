[whisper]
# large-v3 is the best quality model, RTX 4090 can handle it easily
default_model = large-v3
device = cuda
# float16 is optimal for RTX 4090 - good balance of speed and precision
compute_type = float16
# Higher beam size = better quality (5 is good, 8 is better for RTX 4090)
beam_size = 8
# VAD filter improves transcription by detecting voice activity
vad_filter = true
# Best-of sampling: generate multiple candidates, pick best (quality boost)
best_of = 5
# Patience for beam search (higher = more thorough, 1.0-2.0 recommended)
patience = 1.5
# Temperature for sampling (0 = greedy/deterministic, best for accuracy)
temperature = 0
# Compression ratio threshold to detect hallucinations
compression_ratio_threshold = 2.4
# Log probability threshold to filter low-confidence segments
log_prob_threshold = -1.0
# No speech threshold - skip segments likely to be silence
no_speech_threshold = 0.6
# Condition on previous text for better context continuity
condition_on_previous_text = true
# Word-level timestamps for better accuracy
word_timestamps = true

[app]
# Maximum audio duration in seconds (2 hours)
max_duration = 7200
# Bind only to localhost for security (use reverse proxy for external access)
server_name = 127.0.0.1
server_port = 7860
# SECURITY: Never enable share in production - exposes to public internet
share = false

[models]
# All available Whisper models - large-v3 is recommended for RTX 4090
available_models = tiny,base,small,medium,large-v1,large-v2,large-v3

[languages]
available_languages = en,es,fr,de,it,pt,nl,ja,ko,zh,ru,ar,hi,ja,ko

[llm]
# Unified LLM configuration (shared across all backends)
summarize_prompt = Your mission is to create a **detailed and comprehensive summary**.
    
    Before you dive into summarizing, a quick heads-up on the input:
    * If the text looks like a subtitle file (you know the drill: timestamps, short, disconnected lines), first mentally stitch it together into a flowing, continuous narrative. Then, summarize *that* coherent version.
    
    Now, for the summary itself, here's what I'm looking for:
    1.  **Focus on Comprehensive Coverage:** As you generate a more detailed summary, ensure you thoroughly cover the main ideas, key arguments, significant supporting details, important examples or explanations offered in the text, and the overall conclusions or takeaways. Don't just skim the surface.
    2.  **Depth and Desired Length (This is Crucial!):**
        * **Target Range:** Produce a summary that is approximately **10 percent to 25 percent of the original text's length**. For example, if the original text is 1000 words, aim for a summary in the 100-250 word range. If it's 100 lines, aim for 10-25 lines. Use your best judgment to hit this target.
        * **Information Density:** The goal here is not just arbitrary length, but to fill that length with **all genuinely significant information**. Prioritize retaining details that contribute to a deeper understanding of the subject. It's better to include a supporting detail that seems relevant than to omit it and risk losing nuance.
        * **Beyond a Basic Abstract:** This should be much more than a high-level overview. Think of it as creating a condensed version of the text that preserves a good deal of its informative richness and narrative flow. The emphasis is on **thoroughness and completeness of key information** rather than extreme brevity.
    3.  **Accuracy is King:** What you write needs to be a faithful representation of the source material. No making things up, and no injecting your own opinions unless they're explicitly in the text.
    4.  **Clarity and Cohesion:** Even though it's longer, the summary should still be well-organized, clear, and easy to read.
    
    * "Present the summary as a series of well-developed paragraphs."
    * "Give me a detailed summary of approximately [calculate 10-25 percent of expected input length] words."
    * "The summary should be extensive, aiming for about 15 percent of the original content's length."

[vllm]
# vLLM backend (primary, highest performance)
enabled = true
url = http://vllm:8000
# Qwen3-8B: Best model for summarization (May 2025)
# Features: thinking/non-thinking modes, 32K context (131K with YaRN)
# Superior instruction following, fits RTX 4090 24GB with room for KV cache
model = Qwen/Qwen3-8B

[ollama]
enabled = true
url = http://host.docker.internal:11434
# Fallback model when vLLM unavailable
default_model = llama3.1:8b
