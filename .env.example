# YouLama Environment Configuration
# Copy this file to .env and customize

# Ollama Configuration
OLLAMA_HOST=host.docker.internal
OLLAMA_PORT=11434

# Gradio Configuration  
GRADIO_ANALYTICS_ENABLED=false
GRADIO_SERVER_NAME=127.0.0.1
GRADIO_SERVER_PORT=7860

# CUDA/GPU Configuration for RTX 4090
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
CUDA_MODULE_LOADING=LAZY
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Security Settings
# Never set to true in production
SHARE_ENABLED=false
