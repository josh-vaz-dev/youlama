version: '3.8'

services:
  # vLLM inference server (primary LLM backend)
  vllm:
    image: vllm/vllm-openai:latest
    container_name: youlama-vllm
    command:
      - --model=Qwen/Qwen2.5-14B-Instruct
      - --dtype=half
      - --max-model-len=8192
      - --gpu-memory-utilization=0.7
      - --enable-chunked-prefill
      - --disable-log-requests
    networks:
      - youlama_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
        limits:
          memory: 20G
    shm_size: '8g'
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "127.0.0.1:8000:8000"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  youlama:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: youlama-app
    depends_on:
      vllm:
        condition: service_healthy

    # Security: Drop all capabilities, add only what's needed
    cap_drop:
      - ALL

    # Security: No new privileges
    security_opt:
      - no-new-privileges:true

    # Port binding - only localhost for security (use reverse proxy for external access)
    ports:
      - "127.0.0.1:7860:7860"

    # Volumes - use named volumes for persistence
    volumes:
      - youlama_models:/app/models
      - youlama_temp:/app/temp

    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_HOST=host.docker.internal
      # Security: Disable Gradio analytics
      - GRADIO_ANALYTICS_ENABLED=false
      # RTX 4090 optimization
      - CUDA_MODULE_LOADING=LAZY
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

    # GPU access for Whisper transcription
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
        limits:
          memory: 12G

    # Network configuration
    networks:
      - youlama_network
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

    # Restart policy
    restart: unless-stopped

    # Health check
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:7860')" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  youlama_models:
    driver: local
  youlama_temp:
    driver: local

networks:
  youlama_network:
    driver: bridge
